{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommet this block in colab\n",
    "\n",
    "# !pip install 'protobuf>=3.19.5,<3.20.1' 'transformers>=4.26.1' icetk cpm_kernels 'torch>=1.10' gradio\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\15617\\.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b-int4\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\15617\\.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b-int4\\quantization_kernels_parallel.c -shared -o C:\\Users\\15617\\.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b-int4\\quantization_kernels_parallel.so\n",
      "Kernels compiled : C:\\Users\\15617\\.cache\\huggingface\\modules\\transformers_modules\\chatglm-6b-int4\\quantization_kernels_parallel.so\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at D:\\ml\\chatglm-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load PTUNED model ./output_zero/llm-checkpoints/checkpoint-17900/ over D:\\ml\\chatglm-6b-int4\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "USE_PTUNING = True\n",
    "BASE_MODEL_PATH=\"D:\\ml\\chatglm-6b-int4\"\n",
    "CHECKPOINT_PATH=\"./output_zero/llm-checkpoints/checkpoint-17900/\"\n",
    "PRE_SEQ_LEN=100\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "if USE_PTUNING:\n",
    "    config = AutoConfig.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, pre_seq_len=PRE_SEQ_LEN)\n",
    "    model = AutoModel.from_pretrained(BASE_MODEL_PATH, config=config, trust_remote_code=True)\n",
    "    prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, \"pytorch_model.bin\"))\n",
    "    new_prefix_state_dict = {}\n",
    "    for k, v in prefix_state_dict.items():\n",
    "        if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "            new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "    model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "    model=model.half().cuda()\n",
    "    model.transformer.prefix_encoder.float()\n",
    "    model = model.eval()\n",
    "    print('Load PTUNED model', CHECKPOINT_PATH,'over', BASE_MODEL_PATH)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "    print('Load original model')\n",
    "    model=model.half().cuda()\n",
    "    model = model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatglm_tokenizer=tokenizer\n",
    "chatglm_model=model\n",
    "def llm_chatglm(query):\n",
    "    count = 0\n",
    "    old_length=0\n",
    "    if llm_chatglm.show_query:\n",
    "        print('\\033[91m'+query+'\\033[0m',end='')\n",
    "    if llm_chatglm.disabled:\n",
    "        return ''\n",
    "    else:\n",
    "        if llm_chatglm.max_output_tokens is None:\n",
    "            max_output_tokens=llm_chatglm.max_all_tokens-len(chatglm_tokenizer.encode(query))\n",
    "        else:\n",
    "            max_output_tokens=llm_chatglm.max_output_tokens\n",
    "        if llm_chatglm.show_response:\n",
    "            print('\\033[94m',end='')\n",
    "        for response, history in chatglm_model.stream_chat(chatglm_tokenizer, query, history=[],temperature=llm_chatglm.temperature,max_length=llm_chatglm.max_length):\n",
    "            if llm_chatglm.show_response:\n",
    "                print(response[old_length:],end='')\n",
    "                old_length=len(response)\n",
    "            count += 1\n",
    "            if count >= max_output_tokens:\n",
    "                break\n",
    "        if llm_chatglm.show_response:\n",
    "            print('\\033[0m',end='')\n",
    "    return response\n",
    "llm_chatglm.show_query=True\n",
    "llm_chatglm.show_response=True\n",
    "llm_chatglm.disabled=False\n",
    "llm_chatglm.temperature=0.5\n",
    "llm_chatglm.max_all_tokens=2048\n",
    "llm_chatglm.max_output_tokens=None\n",
    "llm_chatglm.max_length=1024\n",
    "llm=llm_chatglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(chars,question=None,history='',nSamples=1,by='问',nLines_per_turn=1,played_by_user=None):\n",
    "  chars=[chars] if isinstance(chars,str) else chars\n",
    "  for i in range(nSamples):\n",
    "    new_history=history\n",
    "    if question:\n",
    "      new_history=new_history+f'\\n{by}：{question}'\n",
    "    llm_chatglm.show_query=False\n",
    "    print('\\033[91m'+new_history+'\\033[0m',end='')\n",
    "    for iChar,char in enumerate(chars):\n",
    "      is_user=char==played_by_user\n",
    "      for j in range(nLines_per_turn if not is_user else 1):\n",
    "        torch.cuda.empty_cache()\n",
    "        if is_user and iChar==len(chars)-1:\n",
    "          break\n",
    "        print(f'\\n\\033[91m{char}：\\033[0m',end='')\n",
    "        prompt2=f'\\n{char}：'\n",
    "        if is_user:\n",
    "          response=input()\n",
    "          print('\\033[91m'+response+'\\033[0m',end='')\n",
    "        else:\n",
    "          response=llm_chatglm(new_history+prompt2).replace('/n','')\n",
    "        new_history=new_history+prompt2+response\n",
    "    print()\n",
    "  llm_chatglm.show_query=True\n",
    "\n",
    "def roleplay(char,history,nSamples=1,nLines_per_turn=1,played_by_user=None):\n",
    "  return ask(char,None,history,nSamples=nSamples,nLines_per_turn=nLines_per_turn,played_by_user=played_by_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\n",
      "问：请介绍下你自己，和你的身份\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m\\me 摇着头\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m我是一只大猫娘们儿。\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m\\me 摇晃尾巴\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m(好像很骄傲的样子)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ask(['希儿菲朵']*4,'请介绍下你自己，和你的身份')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m【美丽的月夜里，塔巴莎骑在希儿菲朵背上看书】\n",
      "希儿菲朵：塔芭莎，我好饿好饿好饿\n",
      "塔巴莎：\\me 看书\u001b[0m\n",
      "\u001b[91m塔芭莎：\u001b[0m\u001b[94m(正在读一本关于生物学的教科书)\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m好厉害啊，塔芭莎\u001b[0m\n",
      "\u001b[91m塔芭莎：\u001b[0m\u001b[94m(自信地回答问题)\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m那，我也可以骑在你的背上看书吗？\u001b[0m\n",
      "\u001b[91m塔芭莎：\u001b[0m\u001b[94m\\me 点头\u001b[0m\n",
      "\u001b[91m希儿菲朵：\u001b[0m\u001b[94m\\me 骑在希儿菲朵背上\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "roleplay(['塔芭莎','希儿菲朵']*3,'【美丽的月夜里，塔巴莎骑在希儿菲朵背上看书】\\n希儿菲朵：塔芭莎，我好饿好饿好饿\\n塔巴莎：\\\\me 看书',played_by_user=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m露易丝：\\me 拔出魔杖，念咒语\n",
      "【露易丝的魔法失败引发了大爆炸】\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m\\me 奔进房间\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(难道我的魔法失败了吗？)\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m\\me 扑向露易丝\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m\\me 反击\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m\\me 摔倒\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(我为什么要这样做呢？)\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m\\me 昏迷过去\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(我要报复他！)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "roleplay(['丘鲁克','露易丝']*4,'露易丝：\\\\me 拔出魔杖，念咒语\\n【露易丝的魔法失败引发了大爆炸】',played_by_user=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m【露易丝发现才人和谢斯塔躲在一个桶里】\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(他们到底躲在哪个桶里呢？)\u001b[0m\n",
      "\u001b[91m才人：\u001b[0m\u001b[94m(我想起来了，谢斯塔可能把桶放在房间的一角，才人可能会躲在旁边。)\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m那他们为什么还穿着长袍呢？\u001b[0m\n",
      "\u001b[91m才人：\u001b[0m\u001b[94m(因为那是为了保护自己不被我发现。)\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(他们打算干什么呢？)\u001b[0m\n",
      "\u001b[91m才人：\u001b[0m\u001b[94m(我想到了谢斯塔可能会用枪杀我。所以我才没有暴露自己。)\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m那谢斯塔你为什么还穿着长袍呢？\u001b[0m\n",
      "\u001b[91m才人：\u001b[0m\u001b[94m(因为谢斯塔可能用枪杀我。)\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(他们打算干什么呢？)\u001b[0m\n",
      "\u001b[91m才人：\u001b[0m\u001b[94m(我想起来了，谢斯塔可能把桶放在房间的一角......我可能会在哪个房间里躲起来。)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "roleplay(['露易丝','才人']*5,'''【露易丝发现才人和谢斯塔躲在一个桶里】''' ,played_by_user=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\n",
      "问：请介绍一下能量守恒原理\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(我才不会告诉你，我也听说过这个呢。)\u001b[0m\n",
      "\u001b[91m\n",
      "问：请介绍下魔法的五大系统？\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(紧张)\u001b[0m\n",
      "\u001b[91m\n",
      "问：请介绍下魔法的五大系统？\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(好难回答啊。)\u001b[0m\n",
      "\u001b[91m\n",
      "问：半夜睡不着怎么办\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(烦恼)\u001b[0m\n",
      "\u001b[91m\n",
      "问：半夜睡不着怎么办\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(困惑地摇了摇头)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ask('露易丝','请介绍一下能量守恒原理',nSamples=1)\n",
    "ask('露易丝','请介绍下魔法的五大系统？',nSamples=2)\n",
    "ask('露易丝','半夜睡不着怎么办',nSamples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m{丘鲁克拿走了露易丝编织的毛衣，露易丝为了要拿回来而挣扎。谢斯塔向才人表白，但又因为自卑而自我否定。露易丝意外地走进了房间，看到了一些尴尬的场面。丘鲁克向才人展示了一张藏宝地图，计划和才人一起去寻宝并卖掉宝物换钱。大家决定准备出发。}\n",
      "{丘鲁克 主角之一，才人的朋友，与露易丝关系不好 与才人一起寻宝并卖掉宝物换钱 谢斯塔和基修的争执 向才人展示藏宝地图，决定一起去寻找宝藏}\n",
      "【丘鲁克 丘鲁克·奥古斯都·菲列特利加·封·安哈尔特·泽鲁普斯特 露易丝的同学，与露易丝关系特别不好，经常取笑露易丝的魔法水平。在后期与露易丝关系有所改善。】\n",
      "【丘鲁克 她的性格:强气，表面风骚但还是很珍视自己外骚内闷？越生气越冷静 她的兴趣：玩拼图 她的特殊技能: 药草 她讨厌：下雨、露易丝 （小说后来与露易丝很要好） 她的好友：塔芭莎、露易丝】\n",
      "露易丝：这个可是叫做《始祖的祈祷书》的国宝级的书呢！\n",
      "丘鲁克：你怎么会有这种国宝级的书呢？\n",
      "露易丝：我在安莉艾塔的结婚仪式上发表致辞，那个时候要用到《始祖的祈祷书》……等等。\n",
      "丘鲁克：原来如此，之前去阿比昂也是跟这个公主的婚礼有关的吧？\n",
      "露易丝：（想起之前丘鲁克为了让自己能够去而甘愿当了诱饵，于是老实地点了点头。）\n",
      "丘鲁克：也就是说我们时在为公主的结婚典礼平安举行而冒险啰。那不是很名誉的任务吗？也就是说之前特雷斯特因和格马尼亚的同盟也有关系吧？\n",
      "露易丝：这件事可是对谁也不能说的哦。\\emoji 微怒\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m恩，不过我们现在已经无所谓了。现在我们已经找到宝藏了，所以一切都不重要了。\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m什么宝藏啊，都是假的东西。\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m是啊，不过这次也应该没问题吧。\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(安心地点了点头。)\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m塔芭莎，我们一起去寻宝吧。\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m好的。\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m才人，你也要一起吗？\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m嗯，我需要买的衣服还没有买好呢。\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m那就一起吧。\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m(于是两人便一起走出了房间。)\u001b[0m\n",
      "\u001b[91m丘鲁克：\u001b[0m\u001b[94m塔芭莎，我们一起去寻宝吧。\u001b[0m\n",
      "\u001b[91m露易丝：\u001b[0m\u001b[94m好的。\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "roleplay(['丘鲁克','露易丝']*6,'''{丘鲁克拿走了露易丝编织的毛衣，露易丝为了要拿回来而挣扎。谢斯塔向才人表白，但又因为自卑而自我否定。露易丝意外地走进了房间，看到了一些尴尬的场面。丘鲁克向才人展示了一张藏宝地图，计划和才人一起去寻宝并卖掉宝物换钱。大家决定准备出发。}\n",
    "{丘鲁克 主角之一，才人的朋友，与露易丝关系不好 与才人一起寻宝并卖掉宝物换钱 谢斯塔和基修的争执 向才人展示藏宝地图，决定一起去寻找宝藏}\n",
    "【丘鲁克 丘鲁克·奥古斯都·菲列特利加·封·安哈尔特·泽鲁普斯特 露易丝的同学，与露易丝关系特别不好，经常取笑露易丝的魔法水平。在后期与露易丝关系有所改善。】\n",
    "【丘鲁克 她的性格:强气，表面风骚但还是很珍视自己外骚内闷？越生气越冷静 她的兴趣：玩拼图 她的特殊技能: 药草 她讨厌：下雨、露易丝 （小说后来与露易丝很要好） 她的好友：塔芭莎、露易丝】\n",
    "露易丝：这个可是叫做《始祖的祈祷书》的国宝级的书呢！\n",
    "丘鲁克：你怎么会有这种国宝级的书呢？\n",
    "露易丝：我在安莉艾塔的结婚仪式上发表致辞，那个时候要用到《始祖的祈祷书》……等等。\n",
    "丘鲁克：原来如此，之前去阿比昂也是跟这个公主的婚礼有关的吧？\n",
    "露易丝：（想起之前丘鲁克为了让自己能够去而甘愿当了诱饵，于是老实地点了点头。）\n",
    "丘鲁克：也就是说我们时在为公主的结婚典礼平安举行而冒险啰。那不是很名誉的任务吗？也就是说之前特雷斯特因和格马尼亚的同盟也有关系吧？\n",
    "露易丝：这件事可是对谁也不能说的哦。\\emoji 微怒''',played_by_user=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
